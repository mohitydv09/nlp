{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mohit Yadav\n",
    "## yadav171@umn.edu\n",
    "## CSCI 5541 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/rpmdt05/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torchtext\u001b[38;5;241m.\u001b[39mdisable_torchtext_deprecation_warning()\n",
      "File \u001b[0;32m~/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/mambaforge/envs/mohit-new-nlp/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/rpmdt05/mambaforge/envs/mohit-new-nlp/lib/python3.8/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
     ]
    }
   ],
   "source": [
    "## Importing packages.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set device to Cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download IMDB data.\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of html in the text, and there are symbols which don't have meaning for our work. So we will need to clean the data before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the data to a pandas df for easy processing.\n",
    "train_df = pd.DataFrame(train_iter, columns=['label', 'text'])\n",
    "test_df = pd.DataFrame(test_iter, columns=['label', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See the text stored in the dataframe.\n",
    "print(train_df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of symbols and html text in the sample that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the text and store in a txt file for training the tokenizer.\n",
    "with open(\"cleaned_text.txt\", \"w\") as f:\n",
    "    for label, text in train_iter:\n",
    "        f.write(clean_text(text).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Tokenization.\n",
    "## Create spm model with our data and vocab size of 8000.\n",
    "VOCAB_SIZE = 8000\n",
    "spm.SentencePieceTrainer.train(input='./cleaned_text.txt', model_prefix='bpe_model', vocab_size=VOCAB_SIZE, model_type='bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='bpe_model.model')\n",
    "\n",
    "## Tokenization functions.\n",
    "def tokenize_into_str(text):\n",
    "    return \" \".join(sp.encode(text, out_type=str))\n",
    "\n",
    "def tokenize_into_idx(text):\n",
    "    return \" \".join([str(tok) for tok in sp.encode(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add cleaned text to dataframes.\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "train_df['tokenized_text'] = train_df['cleaned_text'].apply(tokenize_into_str)\n",
    "train_df['tokenized_idx'] = train_df['cleaned_text'].apply(tokenize_into_idx)\n",
    "\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "test_df['tokenized_text'] = test_df['cleaned_text'].apply(tokenize_into_str)\n",
    "test_df['tokenized_idx'] = test_df['cleaned_text'].apply(tokenize_into_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the dataframe for idx and nan values.\n",
    "train_df.replace({\"tokenized_idx\": \"\"}, np.nan, inplace=True)\n",
    "train_df.dropna(subset=['tokenized_idx'], inplace=True)\n",
    "\n",
    "test_df.replace({\"tokenized_idx\": \"\"}, np.nan, inplace=True)\n",
    "test_df.dropna(subset=['tokenized_idx'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the size of tokens to df, will be used by model.\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train_df['num_tokens'] = train_df['tokenized_idx'].apply(count_tokens)\n",
    "test_df['num_tokens'] = test_df['tokenized_idx'].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See sample of data.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stats of number of tokens in data.\n",
    "print(f\"Train:\\t mean={train_df['num_tokens'].mean():.2f}, std={train_df['num_tokens'].std():.2f}, max={train_df['num_tokens'].max():.2f}, min={train_df['num_tokens'].min():.2f}\")\n",
    "print(f\"Test:\\t mean={test_df['num_tokens'].mean():.2f}, std={test_df['num_tokens'].std():.2f}, max={test_df['num_tokens'].max():.2f}, min={test_df['num_tokens'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of variation in token size as seen from the high std values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove non relevant data from the dfs for training.\n",
    "train_df = train_df[['tokenized_idx', 'num_tokens', 'label']]\n",
    "test_df = test_df[['tokenized_idx', 'num_tokens', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the labels 0 and 1.\n",
    "train_df.loc[:,'label'] = train_df['label'] - 1\n",
    "test_df.loc[:,'label'] = test_df['label'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check to see appropriate conversion.\n",
    "print(\"Unique Labels in train data: \",train_df['label'].unique())\n",
    "print(\"Unique Labels in test data: \",test_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the DataLoader Class,\n",
    "## I am not reading from disk as all the data was already loaded into RAM.\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_indices = [int(token_idx) for token_idx in self.dataset.iloc[idx]['tokenized_idx'].split()]\n",
    "        number_of_tokens = self.dataset.iloc[idx]['num_tokens']\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        return text_indices, number_of_tokens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = 2\n",
    "\n",
    "## Set Seed\n",
    "torch.manual_seed(33)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a Single layer model.\n",
    "class SLMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding_sum = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding_sum.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        return self.fc(self.embedding_sum(text, offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the multi layer model.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding_sum = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding_sum.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        x = self.embedding_sum(text, offsets)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to generate data in a batch.\n",
    "def generate_batch(batch):\n",
    "    batch_indices = []\n",
    "    batch_labels = []\n",
    "    offsets = [0]\n",
    "\n",
    "    for text_indices, number_of_tokens, label in batch:\n",
    "        batch_indices.extend(text_indices)\n",
    "        batch_labels.append(label)\n",
    "        offsets.append(number_of_tokens)\n",
    "\n",
    "    batch_indices = torch.tensor(batch_indices, dtype=torch.long)\n",
    "    batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    return batch_indices, batch_labels, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define DataLoaders.\n",
    "train_loader = DataLoader(imdbDataset(train_df), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_loader = DataLoader(imdbDataset(test_df), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Training Function\n",
    "def train_func(data_loader, model, criterion, optimizer, scheduler):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    total_size = 0\n",
    "    for i, (text_indices, label, number_of_tokens) in enumerate(data_loader):\n",
    "        total_size += len(label)\n",
    "        optimizer.zero_grad()\n",
    "        text_indices, number_of_tokens, label = text_indices.to(device), number_of_tokens.to(device), label.to(device)\n",
    "\n",
    "        # Forward pass.\n",
    "        model_output = model(text_indices, number_of_tokens)\n",
    "\n",
    "        ## Compute loss and accuracy.\n",
    "        loss = criterion(model_output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (model_output.argmax(dim=1) == label).sum().item()\n",
    "    scheduler.step()\n",
    "    return train_loss / total_size, train_acc / total_size\n",
    "\n",
    "\n",
    "## Define Validation Function, can be used on Test data too without any modification.\n",
    "def val_func(data_loader, model, criterion):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_size = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (text_indices, label, number_of_tokens) in enumerate(data_loader):\n",
    "            total_size += len(label)\n",
    "            text_indices, number_of_tokens, label = text_indices.to(device), number_of_tokens.to(device), label.to(device)\n",
    "\n",
    "            # Forward pass.\n",
    "            model_output = model(text_indices, number_of_tokens)\n",
    "\n",
    "            ## Compute loss and accuracy.\n",
    "            loss = criterion(model_output, label)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (model_output.argmax(dim=1) == label).sum().item()\n",
    "    return val_loss / total_size, val_acc / total_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model and training parameters.\n",
    "modelSLMLP = SLMLP(VOCAB_SIZE, EMBED_DIM, NUM_CLASS)\n",
    "\n",
    "## Define Training parameters\n",
    "criterionSLMLP = nn.CrossEntropyLoss()\n",
    "optimizerSLMLP = torch.optim.SGD(modelSLMLP.parameters(), lr=1.0)\n",
    "schedulerSLMLP = torch.optim.lr_scheduler.StepLR(optimizerSLMLP, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Single layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop for single layer model.\n",
    "N_EPOCHS = 31\n",
    "\n",
    "lossSLMLP = []\n",
    "accuracySLMLP = []\n",
    "test_accuracySLMLP = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(train_loader, modelSLMLP, criterionSLMLP, optimizerSLMLP, schedulerSLMLP)\n",
    "    test_loss, test_acc = val_func(test_loader, modelSLMLP, criterionSLMLP)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Test. Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "    lossSLMLP.append(train_loss)\n",
    "    accuracySLMLP.append(train_acc)\n",
    "    test_accuracySLMLP.append(test_acc)\n",
    "\n",
    "print(f\"\\nFinished Training, final accuracy on test data is : {test_accuracySLMLP[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the loss and accuracy\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized measure of loss/accuracy\")\n",
    "x_len = list(range(len(accuracySLMLP)))\n",
    "\n",
    "plt.axis([0,max(x_len),0,1])\n",
    "plt.title(\"Result from Single Layer MLP\")\n",
    "lossSLMLP = np.asarray(lossSLMLP)/max(lossSLMLP)\n",
    "plt.plot(x_len, lossSLMLP, 'r', label='Training loss')\n",
    "plt.plot(x_len, accuracySLMLP, 'b', label='Training acc')\n",
    "plt.plot(x_len, test_accuracySLMLP, 'y', label='Test acc')\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\n",
    "plt.text(31, 0.7, f\"Test Accuracy: {test_accuracySLMLP[-1]*100:.2f}%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron with intermediate layer of size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "modelMLP = MLP(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASS).to(device)\n",
    "\n",
    "## Define Training parameters\n",
    "criterionMLP = nn.CrossEntropyLoss()\n",
    "optimizerMLP = torch.optim.SGD(modelMLP.parameters(), lr=1.0)\n",
    "schedulerMLP = torch.optim.lr_scheduler.StepLR(optimizerMLP, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop for MLP with 100 dim. of intermedialte layer.\n",
    "\n",
    "lossMLP = []\n",
    "accuracyMLP = []\n",
    "test_accuracyMLP = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(train_loader, modelMLP, criterionMLP, optimizerMLP, schedulerMLP)\n",
    "    test_loss, test_acc = val_func(test_loader, modelMLP, criterionMLP)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Test. Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "    lossMLP.append(train_loss)\n",
    "    accuracyMLP.append(train_acc)\n",
    "    test_accuracyMLP.append(test_acc)\n",
    "print(f\"\\nFinished Training, final accuracy on test data is : {test_accuracyMLP[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the loss and accuracy\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized measure of loss/accuracy\")\n",
    "x_len = list(range(len(accuracyMLP)))\n",
    "\n",
    "plt.axis([0,max(x_len),0,1])\n",
    "plt.title(\"Result from MLP with intermediate layer size 100\")\n",
    "lossMLP = np.asarray(lossMLP)/max(lossMLP)\n",
    "plt.plot(x_len, lossMLP, 'r', label='Training loss')\n",
    "plt.plot(x_len, accuracyMLP, 'b', label='Training acc')\n",
    "plt.plot(x_len, test_accuracyMLP, 'y', label='Test acc')\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\n",
    "plt.text(31, 0.7, f\"Test Accuracy: {test_accuracyMLP[-1]*100:.2f}%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision between two-layer MLP to a single layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparision between two-layer MLP to a single layer MLP.\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized measure of loss/accuracy\")\n",
    "x_len = list(range(len(accuracyMLP)))\n",
    "\n",
    "plt.axis([0,max(x_len),0,1])\n",
    "plt.title(\"Comparison between two-layer MLP to a single layer MLP\")\n",
    "plt.plot(x_len, test_accuracySLMLP, 'b', label='Test Accuracy of Single Layer MLP')\n",
    "plt.plot(x_len, test_accuracyMLP, 'g', label='Test Accuracy of Two Layer MLP')\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\n",
    "plt.text(31, 0.7, f\"Test Accuracy Single Layer MLP: {test_accuracySLMLP[-1]*100:.2f}%\\n\\n Test Accuracy Two Layer MLP: {test_accuracyMLP[-1]*100:.2f}%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can see that the test accuracy for the two-layer MLP is slightly lower than that for the single-layer MLP, possibly due to overfitting on the test data. We observe that with the two-layer MLP, the training loss decreases more than it does for the single-layer MLP, but this improvement doesn't carry over to the validation and test data, indicating a problem of overfitting. Since the multi-layer model has significantly more parameters, it attempts to overfit the training data to minimize the training loss as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron with intermediate layer of size 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE2 = 200\n",
    "modelMLP2 = MLP(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE2, NUM_CLASS).to(device)\n",
    "\n",
    "## Define Training parameters\n",
    "criterionMLP2 = nn.CrossEntropyLoss()\n",
    "optimizerMLP2 = torch.optim.SGD(modelMLP2.parameters(), lr=1.0)\n",
    "schedulerMLP2 = torch.optim.lr_scheduler.StepLR(optimizerMLP2, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop for MLP with intermediate layer size 200.\n",
    "\n",
    "lossMLP2 = []\n",
    "accuracyMLP2 = []\n",
    "test_accuracyMLP2 = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(train_loader, modelMLP2, criterionMLP2, optimizerMLP2, schedulerMLP2)\n",
    "    test_loss, test_acc = val_func(test_loader, modelMLP2, criterionMLP2)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Test. Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "    lossMLP2.append(train_loss)\n",
    "    accuracyMLP2.append(train_acc)\n",
    "    test_accuracyMLP2.append(test_acc)\n",
    "print(f\"\\nFinished Training, final accuracy on test data is : {test_accuracyMLP2[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the loss and accuracy\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized measure of loss/accuracy\")\n",
    "x_len = list(range(len(accuracyMLP2)))\n",
    "\n",
    "plt.axis([0,max(x_len),0,1])\n",
    "plt.title(\"Result from MLP with intermediate layer size 200\")\n",
    "lossMLP2 = np.asarray(lossMLP2)/max(lossMLP2)\n",
    "plt.plot(x_len, lossMLP2, 'r', label='Training loss')\n",
    "plt.plot(x_len, accuracyMLP2, 'b', label='Training acc')\n",
    "plt.plot(x_len, test_accuracyMLP2, 'y', label='Test acc')\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\n",
    "plt.text(31, 0.7, f\"Test Accuracy: {test_accuracyMLP2[-1]*100:.2f}%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparision between MLP with intermediate layer size 100 and 200.\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized measure of loss/accuracy\")\n",
    "x_len = list(range(len(accuracyMLP2)))\n",
    "\n",
    "plt.axis([0,max(x_len),0,1])\n",
    "plt.title(\"Comparison between MLP with intermediate layer size 100 and 200\")\n",
    "plt.plot(x_len, test_accuracyMLP, 'b', label='Test Accuracy of MLP with intermediate layer size 100')\n",
    "plt.plot(x_len, test_accuracyMLP2, 'g', label='Test Accuracy of MLP with intermediate layer size 200')\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\n",
    "plt.text(31, 0.7, f\"Test Accuracy of MLP with intermediate layer size 100: {test_accuracyMLP[-1]*100:.2f}%\\n\\n Test Accuracy MLP with intermediate layer size 200: {test_accuracyMLP2[-1]*100:.2f}%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a similar trend as in the comparison between the single-layer MLP and the dual-layer MLP. The MLP with an intermediate layer size of 200 fluctuated more than the one with a size of 100, possibly due to overfitting. Since the model with a 200-sized intermediate layer has greater flexibility to fit the training set, it attempts to reduce training errors excessively, leading to overfitting. That being said, there is no significant difference in the final test accuracies between the models with intermediate sizes of 100 and 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the task, dataset, and hardware used.\n",
    "\n",
    "The task in this assingment was to classify the review of movies into positive or negetive using a Bag of Words approach with Multi layer perceptrons.\n",
    "We used IMDB dataset from torchtext library.\n",
    "For training the model I used my personal M1 macbook air without any GPU. As the models were light I was able to train them on my local machanice in less that 10 mins each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracies\n",
    "\n",
    "The models achieved following test accuracies on the dataset after training.\n",
    "Single layer MLP:\n",
    "Double Layer MPL with intermediate layer of size 100: \n",
    "Double Layer MPL with intermediate layer of size 200: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "I have declared all the hyperparameters as global variables, they are listed below too with values.\n",
    "\n",
    "BATCH_SIZE = 32            ## Tuned to make the model run fast on my local machine.\n",
    "\n",
    "EMBED_DIM = 32             ## Kept same as in the tutorial\n",
    "\n",
    "VOCAB_SIZE = 8000          ## Kept same as in the tutorial\n",
    "\n",
    "HIDDEN_SIZE = 100 or 200   ## as given in problem statement.\n",
    "\n",
    "N_EPOCHS = 26              ## Manually tuned, as I observed that training was almost stagnent after 25 epochs.\n",
    "\n",
    "Learning Rate, lr=1.0      ## Kept same as in the tutorial\n",
    "\n",
    "Scheduler Gamma, gamma = 0.9    ## Kept same as in the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis on the test set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a function to get 5 incorrect predictions.\n",
    "def get_two_incorrect_predictions(data, model):\n",
    "    incorrect_predictions = []\n",
    "    for i, (text_indices, label, number_of_tokens) in enumerate(data):\n",
    "        predictions = model(text_indices, number_of_tokens)\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            if prediction.argmax() != label[i]:\n",
    "                if i == len(predictions) - 1:\n",
    "                    incorrect_predictions.append((text_indices[number_of_tokens[i]:], label[i], prediction.argmax()))\n",
    "                else:\n",
    "                    incorrect_predictions.append((text_indices[number_of_tokens[i]:number_of_tokens[i+1]], label[i], prediction.argmax()))\n",
    "                if len(incorrect_predictions) == 5:\n",
    "                    return incorrect_predictions\n",
    "    return incorrect_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get two incorrect predictions for all the three models.\n",
    "\n",
    "incorrect_predictionsSLMLP = get_two_incorrect_predictions(test_loader, modelSLMLP)\n",
    "incorrect_predictionsMLP = get_two_incorrect_predictions(test_loader, modelMLP)\n",
    "incorrect_predictionsMLP2 = get_two_incorrect_predictions(test_loader, modelMLP2)\n",
    "\n",
    "incorrect_predictions = incorrect_predictionsSLMLP + incorrect_predictionsMLP + incorrect_predictionsMLP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the incorrect predictions to text and print them out.\n",
    "print(\" '0' means negative and '1' means positive.\")\n",
    "for text_indice, actual_label, predicted_label in incorrect_predictions:\n",
    "    decoded_text = sp.decode(text_indice.tolist())\n",
    "    print(\"Actual Label: {} | Predicted Label: {} | Text: {}\".format(actual_label, predicted_label, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of samples with wrong predictions.\n",
    "\n",
    "The samples with incorrect predictions seem a bit unclear to me as well. For instance, in one sample, the user spends a long time discussing all the good aspects of the movie at the beginning, only pointing out the negative parts at the very end. This might explain why the model predicted it as a positive review.\n",
    "\n",
    "Some labels seem ambiguous, as users might write mostly good things about the movie, but the label is still negative, as seen in some samples.\n",
    "\n",
    "A common observation is that many reviews with incorrect predictions describe the movie in great detail rather than focusing on the actual user opinion, which could be the source of the error. A movie’s storyline might include various negative words, but the movie could still be good, or vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
