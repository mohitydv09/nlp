{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import torch\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "## Set the device to train your model.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "\n",
    "## Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=True)\n",
    "\n",
    "## Apply the tokenizer to the dataset.\n",
    "dataset = dataset.map(lambda x: tokenizer(x['sentence'], truncation=True), batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create batch of data using DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create the model.\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a trainer class.\n",
    "class CustomTrainer(Trainer):\n",
    "    def _inner_training_loop(\n",
    "            self,\n",
    "            batch_size = None, \n",
    "            args = None,\n",
    "            resume_from_checkpoint = None,\n",
    "            trial = None,\n",
    "            ignore_keys_for_eval = None\n",
    "    ):\n",
    "        number_of_epochs = args.num_train_epochs\n",
    "        start_time = time.time()\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss_per_epoch = 0\n",
    "            train_acc_per_epoch = 0\n",
    "            with tqdm(train_dataloader, unit = 'batch') as training_epoch:\n",
    "                training_epoch.set_description(f\"Training Epoch {epoch}\")\n",
    "                for step, inputs in enumerate(training_epoch):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = inputs['labels'].to(device)\n",
    "\n",
    "                    ## Forward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    model_outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "                    ## Compute the loss\n",
    "                    loss = criterion(model_outputs['logits'], labels)\n",
    "                    train_loss_per_epoch += loss.item()\n",
    "\n",
    "                    ## Calculate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    ## Update weights\n",
    "                    self.optimizer.step()\n",
    "                    train_accuracy = (model_outputs['logits'].argmax(1) == labels).sum().item()\n",
    "                    train_acc_per_epoch += train_accuracy\n",
    "\n",
    "                    wandb.log({\"Step Training Loss\":loss.item(),\n",
    "                                \"Step Training Accuracy\":train_acc_per_epoch,\n",
    "                                \"Step\":step})\n",
    "\n",
    "            ## Change the learning rate.\n",
    "            self.scheduler.step()\n",
    "\n",
    "            ## Compute the average loss and accuracy over all of the batches.\n",
    "            train_loss_per_epoch /= len(train_dataloader)\n",
    "            train_acc_per_epoch /= (len(train_dataloader) * batch_size)\n",
    "\n",
    "            wandb.log({\"Epoch Train Loss\":train_loss_per_epoch,\n",
    "                        \"Epoch Train Accuracy\":train_acc_per_epoch,\n",
    "                        \"Epoch\":epoch})\n",
    "\n",
    "            ## Run the Model on Evaluation Dataset\n",
    "            eval_loss_per_epoch = 0\n",
    "            eval_acc_per_epoch = 0\n",
    "            with tqdm(eval_dataloader, unit='batch') as eval_epoch:\n",
    "                eval_epoch.set_description(f\"Evaluation Epoch {epoch}\")\n",
    "                with torch.no_grad():\n",
    "                    for step, inputs in enumerate(eval_epoch):\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = inputs['labels'].to(device)\n",
    "\n",
    "                        ## Foward pass\n",
    "                        model_outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "                        \n",
    "                        ## Compute loss\n",
    "                        loss = criterion(model_outputs['logits'], labels)\n",
    "                        eval_loss_per_epoch += loss.item()\n",
    "\n",
    "                        ## Compute accuracy\n",
    "                        eval_accuracy = (model_outputs['logits'].argmax(1) == labels).sum().item()\n",
    "                        eval_acc_per_epoch += eval_accuracy\n",
    "            \n",
    "            eval_loss_per_epoch /= len(eval_dataloader)\n",
    "            eval_acc_per_epoch /= (len(eval_dataloader) * batch_size)\n",
    "\n",
    "            wandb.log({\"Eval Loss\": eval_loss_per_epoch, \n",
    "                        \"Eval Accuracy\": eval_acc_per_epoch})\n",
    "\n",
    "            print(f'\\tTrain Loss: {train_loss_per_epoch :.3f} | Train Acc: {train_acc_per_epoch*100:.2f}%')\n",
    "            print(f'\\tEval Loss: {eval_loss_per_epoch :.3f} | Eval Acc: {eval_acc_per_epoch*100:.2f}%')\n",
    "        print(f'Time: {(time.time()-start_time)/60:.3f} minutes ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohitydv09\u001b[0m (\u001b[33mmohitydv09-university-of-minnesota5275\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/saturn/Documents/NLP/nlp/wandb/run-20240922_183709-8abhm1zy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2/runs/8abhm1zy' target=\"_blank\">Code Test</a></strong> to <a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2' target=\"_blank\">https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2/runs/8abhm1zy' target=\"_blank\">https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2/runs/8abhm1zy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.54s/batch]\n",
      "Evaluation Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  6.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.667 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.607 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  4.66batch/s]\n",
      "Evaluation Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 41.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.461 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.600 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.78batch/s]\n",
      "Evaluation Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 41.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.347 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.609 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  3.98batch/s]\n",
      "Evaluation Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 27.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.264 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.625 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  3.89batch/s]\n",
      "Evaluation Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 42.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.225 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.639 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  3.51batch/s]\n",
      "Evaluation Epoch 5: 100%|██████████| 1/1 [00:00<00:00, 32.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.197 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.651 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  4.92batch/s]\n",
      "Evaluation Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 43.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.176 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.661 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  4.89batch/s]\n",
      "Evaluation Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 39.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.159 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.670 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  4.81batch/s]\n",
      "Evaluation Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 40.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.149 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.678 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  4.87batch/s]\n",
      "Evaluation Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 16.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.142 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.686 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  4.78batch/s]\n",
      "Evaluation Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 42.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.133 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.692 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  4.44batch/s]\n",
      "Evaluation Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 33.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.124 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.698 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  4.85batch/s]\n",
      "Evaluation Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 41.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.116 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.703 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  4.35batch/s]\n",
      "Evaluation Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 47.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.110 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.707 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  4.92batch/s]\n",
      "Evaluation Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 40.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.104 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.711 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  4.78batch/s]\n",
      "Evaluation Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 40.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.100 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.714 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  4.74batch/s]\n",
      "Evaluation Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 40.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.096 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.716 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  4.61batch/s]\n",
      "Evaluation Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 13.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.093 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.718 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  4.68batch/s]\n",
      "Evaluation Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 39.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.091 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.720 | Eval Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  4.20batch/s]\n",
      "Evaluation Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 38.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.088 | Train Acc: 100.00%\n",
      "\tEval Loss: 0.721 | Eval Acc: 50.00%\n",
      "Time: 0.115 minutes \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024a69b07c3e4c529d3129cc96bf9f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Epoch Train Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Epoch Train Loss</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Eval Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Eval Loss</td><td>▁▁▂▂▃▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>Step</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Step Training Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Step Training Loss</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>19</td></tr><tr><td>Epoch Train Accuracy</td><td>1.0</td></tr><tr><td>Epoch Train Loss</td><td>0.08842</td></tr><tr><td>Eval Accuracy</td><td>0.5</td></tr><tr><td>Eval Loss</td><td>0.72141</td></tr><tr><td>Step</td><td>0</td></tr><tr><td>Step Training Accuracy</td><td>2</td></tr><tr><td>Step Training Loss</td><td>0.08842</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Code Test</strong> at: <a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2/runs/8abhm1zy' target=\"_blank\">https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2/runs/8abhm1zy</a><br/> View project at: <a href='https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2' target=\"_blank\">https://wandb.ai/mohitydv09-university-of-minnesota5275/NPL%20HW2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240922_183709-8abhm1zy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Set wandb parallelist to false.\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'hw2.ipynb'\n",
    "\n",
    "## WandB setup to get plots and metrics.\n",
    "config = dict(\n",
    "    epochs = 5,\n",
    "    classes = 2,\n",
    "    batch_size = 64,\n",
    "    learning_rate = 2e-5,\n",
    "    dataset = 'sst2',\n",
    "    architecture = 'bert'\n",
    ")\n",
    "\n",
    "## Setting up Training Pipeline inside WandB.\n",
    "with wandb.init(project='NPL HW2', name='First Run', config=config):\n",
    "\n",
    "    ## Define Training Arguments.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        num_train_epochs=wandb.config.epochs,\n",
    "        per_device_train_batch_size=wandb.config.batch_size,\n",
    "        per_device_eval_batch_size=wandb.config.batch_size\n",
    "    )\n",
    "\n",
    "    ## Initialize the trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args = training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    ## Train the model\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohit-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
